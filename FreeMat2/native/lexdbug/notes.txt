OK - I need to get the lexer straightened out.  The whole idea of MSCAN appears to work.  But the switching in and out of MScan is broken.  Consider the following simple expression:
[(3) -4], which should lex as [(3),-4] with the comma inserted by MSCAN.  
We start in the scanning state.
Now an '[' is encountered.  We push the scanning state onto the stack,
and switch to MScan.
STACK : Scanning <MScan>
Next, we see a '(', and push the current state onto the stack (Mscan),
and switch to Scanning.
STACK : Scanning MScan <Scanning>
Next, we encounter the "3", and push the current state (Scanning) and
switch to TransposeCheck.
STACK : Scanning MScan Scanning <TransposeCheck>
In state TransposeCheck we see that the next token is ')' (and not
a transpose), so we pop the state stack (which puts us back in Scanning
mode), and push back the ')'.
STACK : Scanning MScan <Scanning>
In state Scanning, we see that the next token is ')'.
The top state is not Scanning, so we move to MScan.
STACK : Scanning <MScan>


On to rewriting the lexer from scratch...  The most important issue remaining is the separation of multiple expressions. This occurs in the case of expressions seperated by a white space inside a row definition, and in one-line if statements, while statements, etc.

So how can we tell if a comma needs to be inserted for an expression to make sense? Lets see...

An expression can terminate with either:
')'
transpose
numeric
string
']'
'}'
ident

An expression can begin with either:
'('
'+'
'-'
'~'
'['
'{'
string
numeric
ident

Thus, if we see an expression terminator followed by an expression 
beginner, we can deduce that there must be a comma between the two 
elements.  Of course, when the beginning term is a '+' we have to make
the same special case checks that were done before...

Now that the floating point number pseudo-parser is working, we turn to
the next stage.  That is the problem of generating an actual token stream.
We can either tokenize the entire stream in one pass, or we can generate tokens
as needed.  The second approach is more efficient.

The lex-on-demand now works correctly.  The next step could be to either: integrage the lexer into FreeMat (probably too early), fix the "virtual comma" issue, or make the lexer robust enough to handle files, etc.

The last one is easy, because I'm going to assume that the size of a program file to be lexed is small relative to the available memory.

1. String translation.
2. Virtual commas
3. Parenthesis matching

On to "virtual commas"... To process a virtual comma, we first need to track when virtual commas can occur.  There are several instances:

1.  Inside square brackets []
2.  Inside curly brackets {}
3.  After a "for"
4.  After a "while"
5.  After an "if"
6.  After an "elseif"
7.  After a "case" statement
8.  NOT inside regular parenthesis ()

We need a stack, because the regular parenthesis (which can be nested) preserve the state of the virtual comma processing flag.  

Remaining for the new lexer.

1. String escape-sequence translation
2. Continuation of lines
3. Special calls
4. two-char operator matches
5. Virtual commas in multi-line bracket expressions...

Here is what is left...

1. String escape-sequence translation
2. Continuation of lines
3. Special calls

How do special calls work?  We are looking for

ident WS other,

where <other> cannot be any token that would result in a (possible) valid
expression.  These are the tokens that are ruled out

other cannot be

.*
+
-
/
\
./
.\
.^
^
>
=
<
~
.'

