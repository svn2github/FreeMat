OK - I need to get the lexer straightened out.  The whole idea of MSCAN appears to work.  But the switching in and out of MScan is broken.  Consider the following simple expression:
[(3) -4], which should lex as [(3),-4] with the comma inserted by MSCAN.  
We start in the scanning state.
Now an '[' is encountered.  We push the scanning state onto the stack,
and switch to MScan.
STACK : Scanning <MScan>
Next, we see a '(', and push the current state onto the stack (Mscan),
and switch to Scanning.
STACK : Scanning MScan <Scanning>
Next, we encounter the "3", and push the current state (Scanning) and
switch to TransposeCheck.
STACK : Scanning MScan Scanning <TransposeCheck>
In state TransposeCheck we see that the next token is ')' (and not
a transpose), so we pop the state stack (which puts us back in Scanning
mode), and push back the ')'.
STACK : Scanning MScan <Scanning>
In state Scanning, we see that the next token is ')'.
The top state is not Scanning, so we move to MScan.
STACK : Scanning <MScan>


On to rewriting the lexer from scratch...  The most important issue remaining is the separation of multiple expressions. This occurs in the case of expressions seperated by a white space inside a row definition, and in one-line if statements, while statements, etc.

So how can we tell if a comma needs to be inserted for an expression to make sense? Lets see...

An expression can terminate with either:
')'
transpose
numeric
string
']'
'}'
ident

An expression can begin with either:
'('
'+'
'-'
'~'
'['
'{'
string
numeric
ident

Thus, if we see an expression terminator followed by an expression 
beginner, we can deduce that there must be a comma between the two 
elements.  Of course, when the beginning term is a '+' we have to make
the same special case checks that were done before...

Now that the floating point number pseudo-parser is working, we turn to
the next stage.  That is the problem of generating an actual token stream.
We can either tokenize the entire stream in one pass, or we can generate tokens
as needed.  The second approach is more efficient.

